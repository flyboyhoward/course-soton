{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline \n",
    "import os\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define fit A mat\n",
    "def designmatpoly(X, wtvector):\n",
    "    X = X[:,0]\n",
    "    A_traint = np.ones(X.shape)\n",
    "    #print (len(wtvector), np.shape(A_traint))    \n",
    "    \n",
    "    for i in range(len(wtvector)-1):\n",
    "        X_row = X**(i + 1)\n",
    "        A_traint = np.vstack((A_traint, X_row))         \n",
    "    A_train = A_traint.T\n",
    "    #print (np.shape(A_train))\n",
    "    return A_train\n",
    "\n",
    "# regularisation gradient descent function\n",
    "def reg_gradientdescent(w, X, y, l2, rate, n_iterations):\n",
    "    wtseq = [w]\n",
    "    for iteration in range(n_iterations):\n",
    "        #print('iteration is:', iteration)\n",
    "        gradients =  2/len(X) * X.T.dot(X.dot(w) - y) + 2 * l2 * w\n",
    "        #print ('gradients',gradients)\n",
    "        w = w - rate * gradients\n",
    "    print ('final gradient',gradients)\n",
    "    return w\n",
    "\n",
    "# squared residuals\n",
    "def squared_residual(x_test, y_test, wt):\n",
    "    residual = 0\n",
    "    x_test = x_test[:,0]\n",
    "    \n",
    "    for i in range(len(x_test)):\n",
    "        y_predi = 0\n",
    "        x_testi = x_test[i]\n",
    "        for wti in range(len(wt)):\n",
    "            y_predi += wt[wti]*np.power(x_testi, wti)\n",
    "        residual += (y_test[i] - y_predi)**2   \n",
    "    #print('residual is:************ ', residual)\n",
    "    return residual\n",
    "\n",
    "# predicted value\n",
    "def predict(x, wt):    \n",
    "    y_pred = []\n",
    "    \n",
    "    for i in range(len(x)):\n",
    "        y_predi = 0\n",
    "        x_i = x[i]\n",
    "        for wti in range(len(wt)):\n",
    "            y_predi += wt[wti]*np.power(x_i, wti)\n",
    "#             print(wt[wti]*np.power(x_i, wti))\n",
    "#             print('---------------------------------')\n",
    "#         print('*************')\n",
    "        y_pred.append(y_predi)\n",
    "#   print('y_pred shape',np.shape(y_pred))\n",
    "    return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare data \n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def f(x):\n",
    "    return np.sin(x)\n",
    "\n",
    "points = 180\n",
    "X = np.linspace(0, 2*np.pi,points)\n",
    "y = f(X)+np.random.normal(0,0.05,points)  # np.sqrt(np.abs(X))*\n",
    "X = np.atleast_2d(X).T\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KFold(n_splits=10, random_state=None, shuffle=False)\n",
      "final gradient [ 0.50048024  0.33512253  0.04816662 -0.25058214  0.0773374  -0.00647845]\n",
      "residual is:  4.339529547495157\n",
      "final gradient [ 0.54649809  0.348196    0.05049825 -0.25650536  0.07853664 -0.00653979]\n",
      "residual is:  4.143558305285801\n",
      "final gradient [ 0.56204439  0.36936165  0.0651984  -0.26413216  0.07899324 -0.00648941]\n",
      "residual is:  4.113940503706317\n",
      "final gradient [ 0.54770342  0.36379387  0.06957538 -0.25901478  0.07684296 -0.00628728]\n",
      "residual is:  4.237152464070675\n",
      "final gradient [ 0.51875495  0.33145016  0.04994943 -0.24348915  0.07420958 -0.00616035]\n",
      "residual is:  4.115950007666658\n",
      "final gradient [ 0.51255212  0.31872875  0.03362417 -0.24824543  0.07791032 -0.00655832]\n",
      "residual is:  4.089770382109116\n",
      "final gradient [ 0.52415736  0.33001813  0.03962605 -0.2548619   0.07910587 -0.00661181]\n",
      "residual is:  4.133383721024568\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "kf = KFold(n_splits = 10)\n",
    "kf.get_n_splits(X)\n",
    "wtseq = []\n",
    "residseq = []\n",
    "print(kf)  \n",
    "w0 = np.ones(6)  # polynomial degree\n",
    "\n",
    "for train_index, test_index in kf.split(X):\n",
    "    \n",
    "    #print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "    A_train = designmatpoly(X_train, w0)\n",
    "    wtfin = reg_gradientdescent(w0, A_train, y_train, 0.5, rate = 0.00000001, n_iterations = 40000000) # take lamda \n",
    "    wtseq.append(wtfin)\n",
    "    resid = squared_residual(X, y, wtfin)\n",
    "    print('residual is: ', resid)\n",
    "    residseq.append(resid)\n",
    "    \n",
    "plt.scatter(X, y, c='b')\n",
    "\n",
    "\n",
    "wtseq = np.asarray(wtseq)\n",
    "resid_min = np.min(residseq)\n",
    "place = np.where(resid_min == np.min(residseq))\n",
    "wtbest = wtseq[place]\n",
    "print('wtbest is: ')\n",
    "ypred = predict(X, wtbest.T)\n",
    "# print('y predict is: ', ypred)\n",
    "plt.plot(X, ypred)\n",
    "print('best wt matix is: ',wtbest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
